{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AfewpZcfzLgR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tfds-nightly\n",
      "  Downloading tfds_nightly-3.2.1.dev202008210105-py3-none-any.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 4.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (4.47.0)\n",
      "Requirement already satisfied, skipping upgrade: promise in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (2.3)\n",
      "Requirement already satisfied, skipping upgrade: termcolor in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-resources; python_version < \"3.9\" in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: absl-py in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=18.1.0 in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-metadata in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (0.22.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: future in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (0.18.2)\n",
      "Requirement already satisfied, skipping upgrade: dill in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (0.3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: dm-tree in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (0.1.5)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (3.12.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.4; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from importlib-resources; python_version < \"3.9\"->tfds-nightly) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos in /opt/conda/lib/python3.7/site-packages (from tensorflow-metadata->tfds-nightly) (1.51.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tfds-nightly) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tfds-nightly) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tfds-nightly) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tfds-nightly) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.6.1->tfds-nightly) (49.1.0.post20200704)\n",
      "Installing collected packages: tfds-nightly\n",
      "  Attempting uninstall: tfds-nightly\n",
      "    Found existing installation: tfds-nightly 3.2.1.dev202008180105\n",
      "    Uninstalling tfds-nightly-3.2.1.dev202008180105:\n",
      "      Successfully uninstalled tfds-nightly-3.2.1.dev202008180105\n",
      "\u001b[33m  WARNING: The script tfds is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed tfds-nightly-3.2.1.dev202008210105\n"
     ]
    }
   ],
   "source": [
    "!pip install -U tfds-nightly --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7S9DAboIJ3wU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf_slim in /opt/conda/lib/python3.7/site-packages (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from tf_slim) (0.8.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py>=0.2.2->tf_slim) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tf_slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kqhBGn3TKo0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocotools in /opt/conda/lib/python3.7/site-packages (2.0.1)\n",
      "Requirement already satisfied: cython>=0.27.3 in /opt/conda/lib/python3.7/site-packages (from pycocotools) (0.29.20)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from pycocotools) (3.2.2)\n",
      "Requirement already satisfied: setuptools>=18.0 in /opt/conda/lib/python3.7/site-packages (from pycocotools) (49.1.0.post20200704)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (1.18.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOsfU9Xv3Mps"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "461lqB2veb-1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "  while \"models\" in pathlib.Path.cwd().parts:\n",
    "    os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "  !git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "object_detection/protos/input_reader.proto:5:1: warning: Import object_detection/protos/image_resizer.proto is unused.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd models/research/\n",
    "protoc object_detection/protos/*.proto --python_out=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/jupyter/hazard-detection/models/research\n",
      "Requirement already satisfied: Pillow>=1.0 in /opt/conda/lib/python3.7/site-packages (from object-detection==0.1) (7.2.0)\n",
      "Requirement already satisfied: Matplotlib>=2.1 in /opt/conda/lib/python3.7/site-packages (from object-detection==0.1) (3.2.2)\n",
      "Requirement already satisfied: Cython>=0.28.1 in /opt/conda/lib/python3.7/site-packages (from object-detection==0.1) (0.29.20)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from Matplotlib>=2.1->object-detection==0.1) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from Matplotlib>=2.1->object-detection==0.1) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from Matplotlib>=2.1->object-detection==0.1) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from Matplotlib>=2.1->object-detection==0.1) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.7/site-packages (from Matplotlib>=2.1->object-detection==0.1) (1.18.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.1->Matplotlib>=2.1->object-detection==0.1) (1.15.0)\n",
      "Building wheels for collected packages: object-detection\n",
      "  Building wheel for object-detection (setup.py): started\n",
      "  Building wheel for object-detection (setup.py): finished with status 'done'\n",
      "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1310302 sha256=ff4ffe6c8f5b479c8162411aaf9539247bc3f6923b6bede63cdecf3cf6b9cc3b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-34ngkkzj/wheels/2f/69/68/d174ba7c18fe52ba5ad48455b812a18ad3d77a17bed688d8be\n",
      "Successfully built object-detection\n",
      "Installing collected packages: object-detection\n",
      "  Attempting uninstall: object-detection\n",
      "    Found existing installation: object-detection 0.1\n",
      "    Uninstalling object-detection-0.1:\n",
      "      Successfully uninstalled object-detection-0.1\n",
      "Successfully installed object-detection-0.1\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cd models/research\n",
    "pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7LPTyzcngy4t"
   },
   "outputs": [],
   "source": [
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CHtRKeWLib4y"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VxFBTsrlg8DF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/account].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set account application-default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pm0HljBV5PCg"
   },
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'kitti',\n",
    "    split=['train', 'test'],\n",
    "    with_info=True,\n",
    "    download=False,\n",
    "#     data_dir='./tensorflow_datasets'\n",
    "    data_dir=\"gs://kitti-dataset-1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z23bQEmKZY3D"
   },
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "  base_url = 'http://download.tensorflow.org/models/object_detection/'\n",
    "  model_file = model_name + '.tar.gz'\n",
    "  model_dir = tf.keras.utils.get_file(\n",
    "    fname=model_name, \n",
    "    origin=base_url + model_file,\n",
    "    untar=True)\n",
    "\n",
    "  model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
    "\n",
    "  model = tf.saved_model.load(str(model_dir))\n",
    "  model = model.signatures['serving_default']\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7porMrE1YnMq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = 'faster_rcnn_resnet50_lowproposals_coco_2018_01_28'\n",
    "# model_name = 'faster_rcnn_resnet101_lowproposals_coco_2018_01_28'\n",
    "\n",
    "detection_model = load_model(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u7AC_EM6dja5"
   },
   "outputs": [],
   "source": [
    "# List of the strings that is used to add correct label for each box.\n",
    "\n",
    "# PATH_TO_LABELS = 'models/research/object_detection/data/kitti_label_map.pbtxt'\n",
    "PATH_TO_LABELS = 'models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZvLqBFuq5h3"
   },
   "outputs": [],
   "source": [
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vh6bGDVdOt7U"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: {image: (None, None, 3), image/file_name: (), objects: {alpha: (None,), bbox: (None, 4), dimensions: (None, 3), location: (None, 3), occluded: (None,), rotation_y: (None,), truncated: (None,), type: (None,)}}, types: {image: tf.uint8, image/file_name: tf.string, objects: {alpha: tf.float32, bbox: tf.float32, dimensions: tf.float32, location: tf.float32, occluded: tf.int64, rotation_y: tf.float32, truncated: tf.float32, type: tf.int64}}>"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCEmzEc_rGdN"
   },
   "outputs": [],
   "source": [
    "\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GOyGpImZOe5-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'image_tensor:0' shape=(None, None, None, 3) dtype=uint8>]\n"
     ]
    }
   ],
   "source": [
    "print(detection_model.inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hRQW2QSmd_vf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_detections': tf.float32,\n",
       " 'detection_boxes': tf.float32,\n",
       " 'detection_classes': tf.float32,\n",
       " 'detection_scores': tf.float32}"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_model.output_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FMI1eV4-eCY2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_detections': TensorShape([None]),\n",
       " 'detection_boxes': TensorShape([None, 20, 4]),\n",
       " 'detection_classes': TensorShape([None, 20]),\n",
       " 'detection_scores': TensorShape([None, 20])}"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_model.output_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output format\n",
    "\n",
    "\n",
    "- Output classes are always integers in the range 0, num_classes). Any mapping of these integers to semantic labels is to be handled outside of this class. We never explicitly emit a “background class” --- thus 0 is the first non-background class and any logic of predicting and removing implicit background classes must be handled internally by the implementation.\n",
    "\n",
    "\n",
    "- Detected boxes are to be interpreted as being in (y_min, x_min, y_max, x_max) format and normalized relative to the image window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z56oS5LSg8D_"
   },
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image):\n",
    "  image = np.asarray(image)\n",
    "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "  input_tensor = tf.convert_to_tensor(image)\n",
    "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "  input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "  # Run inference\n",
    "  output_dict = model(input_tensor)\n",
    "\n",
    "  # All outputs are batches tensors.\n",
    "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "  # We're only interested in the first num_detections.\n",
    "  num_detections = int(output_dict.pop('num_detections'))\n",
    "  output_dict = {key:value[0, :num_detections].numpy() \n",
    "                 for key,value in output_dict.items()}\n",
    "  output_dict['num_detections'] = num_detections\n",
    "\n",
    "  # detection_classes should be ints.\n",
    "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "   \n",
    "  # Handle models with masks:\n",
    "  if 'detection_masks' in output_dict:\n",
    "    # Reframe the the bbox mask to the image size.\n",
    "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "               image.shape[0], image.shape[1])      \n",
    "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
    "                                       tf.uint8)\n",
    "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " bbox: RawBoundingBox, bounding box in Kitti coordinates (origin top left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(data):\n",
    "    height= 375\n",
    "    width = 1242\n",
    "    \n",
    "    data.loc[:,'xmin'] = data['xmin'] / width \n",
    "    data.loc[:,'xmax'] = data['xmax'] / width\n",
    "    data.loc[:,'ymax'],data.loc[:,'ymin']  = (height - data['ymin']) / height , (height - data['ymax']) / height\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"model@1597671595\"\n",
    "WEIGHTS = \"model@1597671595\"\n",
    "\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('generated_files/{}.json'.format(MODEL), 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json( loaded_model_json )\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"generated_files/{}.h5\".format(WEIGHTS))\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "loaded_model.compile(loss='mean_squared_error', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categories = [{\n",
    "        'id': 1,\n",
    "        'name': 'car'\n",
    "    }, {\n",
    "        'id': 2,\n",
    "        'name': 'van'\n",
    "    }, {\n",
    "        'id': 3,\n",
    "        'name': 'truck'\n",
    "    }, {\n",
    "        'id': 4,\n",
    "        'name': 'pedestrian'\n",
    "    }]\n",
    "\n",
    "cat_index  = {i+1: val for i,val in enumerate(categories) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_locnet( bboxes):\n",
    "  \n",
    "    if len(bboxes)==0 or len(bboxes)==1 and not any(bboxes[0]):\n",
    "        return []\n",
    "    y_pred = loaded_model.predict(bboxes)\n",
    "  \n",
    "    return np.hstack((bboxes,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_kitti(classes):\n",
    "    \n",
    "    hash = { 3:1 , 8:3, 1:4, 7:7 }\n",
    "    return [ hash.get(classes[i], 8) for i in range(len(classes)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_detections( output_dict):\n",
    "    \n",
    "    output_dict['detection_classes'] = transform_to_kitti(output_dict['detection_classes'])\n",
    "    \n",
    "#     print(output_dict  )\n",
    "    misc_ids = (8,9)\n",
    "    \n",
    "    scores = output_dict['detection_scores']\n",
    "    classes = output_dict['detection_classes']\n",
    "    \n",
    "    size = len(classes)\n",
    "    min_threshhold = 0.5\n",
    "    output_dict['detection_boxes'] = np.array([ output_dict['detection_boxes'][i] for i in range(size) if scores[i] >= min_threshhold and classes[i] not in misc_ids  ])\n",
    "    output_dict['detection_classes'] = np.array([ output_dict['detection_classes'][i] for i in range(size) if scores[i] >= min_threshhold and classes[i] not in misc_ids  ])\n",
    "    output_dict['detection_scores'] = np.array([ output_dict['detection_scores'][i] for i in range(size) if scores[i] >= min_threshhold and classes[i] not in misc_ids  ])\n",
    "\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_up_boxes( output_dict):\n",
    "    \n",
    "    output_dict['detection_boxes'] = output_dict['detection_boxes'] * 1000\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filterpy in /opt/conda/lib/python3.7/site-packages (1.4.5)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from filterpy) (3.2.2)\n",
      "Requirement already satisfied: scipy in /home/jupyter/.local/lib/python3.7/site-packages (from filterpy) (1.4.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from filterpy) (1.18.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->filterpy) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->filterpy) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->filterpy) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->filterpy) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib->filterpy) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install filterpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Sort object\n",
    "from sort import *\n",
    "mot_tracker = Sort( max_age=8, iou_threshold=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_for_sort(array):\n",
    "\n",
    "     return np.array( [array[1] ,array[0], array[3], array[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_detections_for_mot(outputdict):\n",
    "    \n",
    "    detections = [ np.append( reorder_for_sort(outputdict['detection_boxes'][i]) , outputdict['detection_scores'][i])  for i in range( len(outputdict['detection_classes'])) ] \n",
    "    \n",
    "    return np.asarray(detections)  if len(detections) else np.empty((0, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_boxes( boxes):\n",
    "    \n",
    "    result = boxes.numpy()\n",
    "    for box in result:\n",
    "        box[0]=1-box[0]\n",
    "        box[2]=1-box[2]\n",
    "        box[0],box[2] = box[2], box[0]\n",
    "    return tf.convert_to_tensor(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tracked_color(label):\n",
    "    \n",
    "    return vis_util.STANDARD_COLORS[label %len(vis_util.STANDARD_COLORS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length(x1,y1,x2,y2):\n",
    "    return sqrt( (x2-x1)**2 + (y2-y1)**2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import cv2\n",
    "import PIL.Image as Image\n",
    "import PIL.ImageColor as ImageColor\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "import PIL.ImageFont as ImageFont\n",
    "from math import sqrt\n",
    "\n",
    "def process_image(image, objects=None, groundtruth=None, mode=None):\n",
    "  \n",
    "\n",
    "    image_np = np.copy(image)\n",
    "    \n",
    "    output_dict = run_inference_for_single_image(detection_model, image_np)\n",
    "    \n",
    "    output_dict =filter_detections( output_dict)\n",
    "\n",
    "    max_age=25\n",
    "    global frame\n",
    "    global warning_ids\n",
    "    thickness=1\n",
    "    if mode =='tracking':\n",
    "        \n",
    "        detections = format_detections_for_mot(output_dict)\n",
    "\n",
    "        tracked_objects = mot_tracker.update(detections)\n",
    "        live_trackers = (i for i in mot_tracker.trackers if i.id+1 in (int(j[4]) for j in tracked_objects) )\n",
    "        \n",
    "#         warning_ids =[]\n",
    "        for kt in live_trackers:\n",
    "            \n",
    "            k = copy.deepcopy(kt)\n",
    "            center = convert_bbox_to_z(k.get_state()[0]).T[0]\n",
    "\n",
    "            for x in range(60):\n",
    "                predicted = k.predict()\n",
    "\n",
    "            xmin,ymin,xmax,ymax = predicted[0] \n",
    "#             track_id = k.id+1\n",
    "#             label = str(track_id)\n",
    "#             vis_util.draw_bounding_box_on_image_array(image_np, ymin, xmin, ymax, xmax, thickness=1, display_str_list=[label],color=get_tracked_color(track_id))\n",
    "\n",
    "            predicted_center = convert_bbox_to_z(predicted[0]).T[0]\n",
    "            \n",
    "            if center[2] < 0.4:\n",
    "\n",
    "                image = Image.fromarray(image_np)\n",
    "                draw = ImageDraw.Draw(image)\n",
    "                im_width, im_height = image.size\n",
    "\n",
    "\n",
    "                (x1,y1) = center[:2]\n",
    "                (x2,y2) = predicted_center[:2]\n",
    "\n",
    "                (x1_absolute, x2_absolute, y1_absolute, y2_absolute) = (x1 * im_width, x2 * im_width,\n",
    "                                      y1 * im_height, y2 * im_height)\n",
    "\n",
    "                draw.line([(x1_absolute, y1_absolute), (x2_absolute, y2_absolute)],\n",
    "                          width=3,\n",
    "                          fill='red')\n",
    "\n",
    "                np.copyto(image_np, np.array(image))\n",
    "            \n",
    "                #hazard detection\n",
    "                if 0.2 < predicted_center[0] < 0.8 and 0.75 < predicted_center[1] and length(x1,y1,x2,y2) > sqrt(center[2])*0.6:\n",
    "                    warning_ids.append( (k.id+1,frame+max_age))\n",
    "            \n",
    "            \n",
    "        warning_ids = [i for i in warning_ids if i[1]>frame ] #Warnings should persit for max_age frames, filter expired warnings here\n",
    "        \n",
    "        for object in tracked_objects:\n",
    "            xmin,ymin,xmax,ymax = object[:4]\n",
    "            track_id = int(object[4])\n",
    "#             n= track_id%40 +1\n",
    "            n= track_id\n",
    "            label = f'object{n:03}'\n",
    "            \n",
    "            color = get_tracked_color(track_id)\n",
    "            \n",
    "            if convert_bbox_to_z(object).T[0][2] < 0.4:\n",
    "                if track_id in (i[0] for i in warning_ids ):\n",
    "                    color = 'red'\n",
    "                    label = 'WARNING!'\n",
    "                    thickness=4\n",
    "                vis_util.draw_bounding_box_on_image_array(image_np, ymin, xmin, ymax, xmax, thickness=thickness, display_str_list=[label],color=color)\n",
    "\n",
    "\n",
    "    elif mode=='locations':\n",
    "        locations = run_locnet( output_dict['detection_boxes'])\n",
    "\n",
    "        i=0\n",
    "        while i < len(objects['location']) and i < len(locations):\n",
    "            ymin,xmin,ymax,xmax = locations[i][:4]\n",
    "    #             print( objects['location'].numpy() , i)\n",
    "            coords = locations[i][4:7:2]\n",
    "            groundtruth_coords = objects['location'].numpy()[i][0:3:2]\n",
    "            label = \"predicted:(%.1f,%.1f), actual:(%.1f,%.1f)\" % tuple(np.append(coords  , groundtruth_coords).tolist())\n",
    "            vis_util.draw_bounding_box_on_image_array(image_np, ymin, xmin, ymax, xmax, thickness=1, display_str_list=[label],color='green')\n",
    "            i+=1\n",
    "\n",
    "            \n",
    "    else:\n",
    "        # Visualization of the results of a detection.\n",
    "\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np,\n",
    "          output_dict['detection_boxes'],\n",
    "          output_dict['detection_classes'],\n",
    "          output_dict['detection_scores'],\n",
    "          cat_index,\n",
    "          use_normalized_coordinates=True,\n",
    "          line_thickness=2)\n",
    "\n",
    "\n",
    "        if groundtruth:\n",
    "            groundtruth_boxes = format_boxes(objects['bbox']).numpy()\n",
    "            groundtruth_classes = objects['type'].numpy()+1\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np,\n",
    "          groundtruth_boxes,\n",
    "          groundtruth_classes,\n",
    "            None,\n",
    "          cat_index,\n",
    "          use_normalized_coordinates=True,\n",
    "          line_thickness=1,\n",
    "            groundtruth_box_visualization_color='blue')\n",
    "        \n",
    "    frame +=1\n",
    "    return image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41085333,  0.4335749 ,  0.5223467 ,  0.4712963 , -0.75153714,\n",
       "         1.01438296, 28.55563545]])"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_locnet( [[0.41085333,0.4335749 ,0.5223467 ,0.4712963 ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with -6).\n",
       "Contents of stderr:\n",
       "2020-08-21 21:30:34.280623: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
       "Traceback (most recent call last):\n",
       "  File \"/opt/conda/bin/tensorboard\", line 8, in <module>\n",
       "    sys.exit(run_main())\n",
       "  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/main.py\", line 75, in run_main\n",
       "    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n",
       "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 299, in run\n",
       "    _run_main(main, args)\n",
       "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\n",
       "    sys.exit(main(argv))\n",
       "  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/program.py\", line 290, in main\n",
       "    return runner(self.flags) or 0\n",
       "  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/program.py\", line 306, in _run_serve_subcommand\n",
       "    server = self._make_server()\n",
       "  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/program.py\", line 416, in _make_server\n",
       "    ingester.deprecated_multiplexer,\n",
       "  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/backend/application.py\", line 149, in TensorBoardWSGIApp\n",
       "    experimental_middlewares,\n",
       "  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/backend/application.py\", line 257, in __init__\n",
       "    \"Duplicate plugins for name %s\" % plugin.plugin_name\n",
       "ValueError: Duplicate plugins for name projector\n",
       "terminate called without an active exception\n",
       "Fatal Python error: Aborted\n",
       "\n",
       "Current thread 0x00007f7093eb8700 (most recent call first):\n",
       "  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/backend/event_processing/event_file_loader.py\", line 141 in Load\n",
       "  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/backend/event_processing/event_file_loader.py\", line 166 in Load\n",
       "  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/backend/event_processing/event_file_loader.py\", line 192 in Load\n",
       "  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/backend/event_processing/directory_watcher.py\", line 120 in _LoadInternal\n",
       "  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/backend/event_processing/directory_watcher.py\", line 90 in Load\n",
       "  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/backend/event_processing/plugin_event_accumulator.py\", line 181 in Reload\n",
       "  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/backend/event_processing/plugin_event_multiplexer.py\", line 237 in Worker\n",
       "  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/backend/event_processing/plugin_event_multiplexer.py\", line 259 in Reload\n",
       "  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/backend/event_processing/data_ingester.py\", line 104 in _reload\n",
       "  File \"/opt/conda/lib/python3.7/threading.py\", line 870 in run\n",
       "  File \"/opt/conda/lib/python3.7/threading.py\", line 926 in _bootstrap_inner\n",
       "  File \"/opt/conda/lib/python3.7/threading.py\", line 890 in _bootstrap\n",
       "\n",
       "Thread 0x00007f7099f36580 (most recent call first):"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZ32ZyN7g8EA"
   },
   "outputs": [],
   "source": [
    "def show_inference(model, tensor, objects, groundtruth=None, mode=None):\n",
    "  \n",
    "    image_np = np.array(tensor)\n",
    "    image =process_image(image_np, objects, groundtruth, mode)\n",
    "    display(Image.fromarray(image))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for example in ds_test.take(5):  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
    "    image = example[\"image\"]\n",
    "    objects = example[\"objects\"]\n",
    "  \n",
    "    print('bbox:' ,objects['bbox'])\n",
    "    print('location:', objects['location'])\n",
    "    print('type:', objects['type'])\n",
    "    show_inference(detection_model, image, objects, mode='locations')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "711\n"
     ]
    }
   ],
   "source": [
    "print(len(ds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ffJh1zAkg8EC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07558770890645002\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "total = 0\n",
    "for example in ds_test:  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
    "    image = example[\"image\"]\n",
    "    objects = example[\"objects\"]\n",
    "  \n",
    "    start = time.time()\n",
    "    image_np = np.array(image)\n",
    "    run_inference_for_single_image(detection_model, image_np)\n",
    "    end = time.time()\n",
    "    total += (end - start)\n",
    "    \n",
    "print( total/len(ds_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on bbox from tfds\n",
    "\n",
    "bbox: tf.Tensor of type `tf.float32` and shape `[4,]` which contains the\n",
    "      normalized coordinates of the bounding box `[ymin, xmin, ymax, xmax]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'id': 1, 'name': 'car'},\n",
       " 2: {'id': 2, 'name': 'van'},\n",
       " 3: {'id': 3, 'name': 'truck'},\n",
       " 4: {'id': 4, 'name': 'pedestrian'}}"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUALITATIVE EVALUATION STEPS\n",
    "\n",
    "from object_detection import eval_util\n",
    "from object_detection.core import standard_fields as fields\n",
    "from object_detection.metrics import coco_evaluation\n",
    "from object_detection.protos import eval_pb2\n",
    "from object_detection.utils import test_case\n",
    "from object_detection.utils import tf_version\n",
    "\n",
    "input_data_fields = fields.InputDataFields\n",
    "detection_fields = fields.DetectionResultFields\n",
    "\n",
    "\n",
    "for example in ds_test.take(5):  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
    "    image = example[\"image\"]\n",
    "    objects = example[\"objects\"]\n",
    "#     show_inference(detection_model, image)\n",
    "    image_np = np.array(image)\n",
    "    # Actual detection.\n",
    "    output_dict = run_inference_for_single_image(detection_model, image_np)\n",
    "\n",
    "\n",
    "    output_dict = filter_detections( output_dict)\n",
    "\n",
    "    batch_size = 1\n",
    "    key=tf.constant('image1')\n",
    "    \n",
    "    groundtruth_boxes = format_boxes(objects['bbox'])\n",
    "    groundtruth_classes = objects['type']+1\n",
    "    groundtruth = {\n",
    "        input_data_fields.groundtruth_boxes: groundtruth_boxes,\n",
    "        input_data_fields.groundtruth_classes: groundtruth_classes,      \n",
    "    }\n",
    "    \n",
    "    num_detections = tf.constant([len(output_dict['detection_classes'])])\n",
    "    \n",
    "    detections = {\n",
    "        detection_fields.detection_boxes: tf.constant([output_dict['detection_boxes']]) ,\n",
    "        detection_fields.detection_scores: tf.constant([output_dict['detection_scores']]),\n",
    "        detection_fields.detection_classes: tf.constant([output_dict['detection_classes']-1]),\n",
    "        detection_fields.num_detections: num_detections\n",
    "       \n",
    "    }\n",
    "    \n",
    "\n",
    "    image = tf.constant([image_np])\n",
    "    \n",
    "    \n",
    "    result_dict = eval_util.result_dict_for_single_example(image, key,detections, groundtruth)\n",
    "    \n",
    "    side_by_side_img =  vis_util.draw_side_by_side_evaluation_image(result_dict,cat_index)[0][0].numpy()\n",
    "    display(Image.fromarray(side_by_side_img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:image img0 does not have groundtruth difficult flag specified\n",
      "WARNING:root:The following classes have no ground truth examples: [2 3 4]\n",
      "WARNING:py.warnings:/opt/conda/lib/python3.7/site-packages/object_detection/utils/metrics.py:145: RuntimeWarning: invalid value encountered in true_divide\n",
      "  num_images_correctly_detected_per_class / num_gt_imgs_per_class)\n",
      "\n",
      "WARNING:root:The following classes have no ground truth examples: [3 4]\n",
      "WARNING:root:The following classes have no ground truth examples: [3 4]\n",
      "WARNING:root:The following classes have no ground truth examples: [3 4]\n",
      "WARNING:root:The following classes have no ground truth examples: [3 4]\n",
      "WARNING:root:The following classes have no ground truth examples: [3 4]\n",
      "WARNING:root:The following classes have no ground truth examples: [3 4]\n",
      "WARNING:root:The following classes have no ground truth examples: [3 4]\n",
      "WARNING:root:The following classes have no ground truth examples: 3\n",
      "WARNING:root:The following classes have no ground truth examples: 3\n",
      "WARNING:root:The following classes have no ground truth examples: 3\n",
      "WARNING:root:The following classes have no ground truth examples: 3\n",
      "WARNING:root:The following classes have no ground truth examples: 3\n",
      "WARNING:root:The following classes have no ground truth examples: 3\n",
      "WARNING:root:The following classes have no ground truth examples: 3\n",
      "WARNING:root:The following classes have no ground truth examples: 3\n",
      "WARNING:root:The following classes have no ground truth examples: 3\n",
      "WARNING:root:The following classes have no ground truth examples: 3\n",
      "WARNING:root:The following classes have no ground truth examples: 3\n",
      "WARNING:root:The following classes have no ground truth examples: 3\n",
      "WARNING:root:The following classes have no ground truth examples: 3\n",
      "WARNING:root:The following classes have no ground truth examples: 3\n"
     ]
    }
   ],
   "source": [
    "#TODO: Complete Evaluation steps\n",
    "from object_detection.core import standard_fields\n",
    "from object_detection.utils import object_detection_evaluation\n",
    "\n",
    "\n",
    "categories = [{\n",
    "        'id': 1,\n",
    "        'name': 'car'\n",
    "    }, {\n",
    "        'id': 4,\n",
    "        'name': 'pedestrian'\n",
    "    }]\n",
    "\n",
    "pascal_evaluator = object_detection_evaluation.PascalDetectionEvaluator(\n",
    "        categories,matching_iou_threshold=0.5)\n",
    "\n",
    "for index, example in enumerate(ds_test):  \n",
    "    image = example[\"image\"]\n",
    "    objects = example[\"objects\"]\n",
    "  \n",
    "    image_np = np.array(image)\n",
    "    # Actual detection.\n",
    "    output_dict = run_inference_for_single_image(detection_model, image_np)\n",
    "    output_dict = filter_detections( output_dict)\n",
    "    \n",
    "    image_key = 'img'+str(index)\n",
    "    \n",
    "    groundtruth_boxes = format_boxes(objects['bbox']).numpy()\n",
    "    groundtruth_classes = objects['type'].numpy()+1\n",
    "\n",
    "\n",
    "    pascal_evaluator.add_single_ground_truth_image_info(\n",
    "        image_key,\n",
    "        {standard_fields.InputDataFields.groundtruth_boxes: groundtruth_boxes,\n",
    "         standard_fields.InputDataFields.groundtruth_classes:\n",
    "         groundtruth_classes,\n",
    "        })\n",
    "\n",
    "    if len(output_dict['detection_classes']):\n",
    "        pascal_evaluator.add_single_detected_image_info(\n",
    "                image_key,\n",
    "                {standard_fields.DetectionResultFields.detection_boxes: output_dict['detection_boxes'],\n",
    "                 standard_fields.DetectionResultFields.detection_scores:\n",
    "                 output_dict['detection_scores'],\n",
    "                 standard_fields.DetectionResultFields.detection_classes:\n",
    "                 output_dict['detection_classes']\n",
    "                })\n",
    "    \n",
    "\n",
    "    metrics = pascal_evaluator.evaluate()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PascalBoxes_Precision/mAP@0.5IOU': 0.3248299516194694,\n",
       " 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/car': 0.6589698941470521,\n",
       " 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/pedestrian': 0.5965592185610505}"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy in /opt/conda/lib/python3.7/site-packages (1.0.3)\n",
      "Requirement already satisfied: proglog in /opt/conda/lib/python3.7/site-packages (0.1.9)\n",
      "Requirement already satisfied: numpy>=1.17.3; python_version != \"2.7\" in /opt/conda/lib/python3.7/site-packages (from moviepy) (1.18.5)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /opt/conda/lib/python3.7/site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5; python_version >= \"3.4\" in /opt/conda/lib/python3.7/site-packages (from moviepy) (2.9.0)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0; python_version >= \"3.4\" in /opt/conda/lib/python3.7/site-packages (from moviepy) (0.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /opt/conda/lib/python3.7/site-packages (from moviepy) (4.47.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /opt/conda/lib/python3.7/site-packages (from moviepy) (2.24.0)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from imageio<3.0,>=2.5; python_version >= \"3.4\"->moviepy) (7.2.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.8.1->moviepy) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install moviepy proglog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil cp gs://hazard-detection-test-videos/accident_compilation.mp4 accident_compilation.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110M accident_compilation.mp4\n"
     ]
    }
   ],
   "source": [
    "!ls -sh accident_compilation.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video output_faster_rcnn_resnet50_lowproposals_coco_2018_01_281598045598.508329.mp4.\n",
      "Moviepy - Writing video output_faster_rcnn_resnet50_lowproposals_coco_2018_01_281598045598.508329.mp4\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ea629b527c472c9f646daa0eb7a299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='t', max=24741.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-485-d3dd0e701acf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mwhite_clip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mwhite_clip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_videofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mclip1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-182>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mrequires_duration\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attribute 'duration' not set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-181>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36muse_clip_fps_by_default\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m    133\u001b[0m              for (k,v) in k.items()}\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<decorator-gen-180>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mconvert_masks_to_RGB\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_RGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    305\u001b[0m                            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                            \u001b[0mffmpeg_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mffmpeg_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                            logger=logger)\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mremove_temp\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmake_audio\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/moviepy/video/io/ffmpeg_writer.py\u001b[0m in \u001b[0;36mffmpeg_write_video\u001b[0;34m(clip, filename, fps, codec, bitrate, preset, withmask, write_logfile, audiofile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         for t,frame in clip.iter_frames(logger=logger, with_times=True,\n\u001b[0;32m--> 221\u001b[0;31m                                         fps=fps, dtype=\"uint8\"):\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwithmask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36miter_frames\u001b[0;34m(self, fps, with_times, logger, dtype)\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproglog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_bar_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m                 \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-138>\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(f, *a, **kw)\u001b[0m\n\u001b[1;32m     87\u001b[0m         new_kw = {k: fun(v) if k in varnames else v\n\u001b[1;32m     88\u001b[0m                  for (k,v) in kw.items()}\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m#mf = copy(self.make_frame)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mnewclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_make_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(gf, t)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \"\"\"\n\u001b[1;32m    489\u001b[0m         \u001b[0mapply_to\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_to\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;31m# --------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-468-e5994c93b6fe>\u001b[0m in \u001b[0;36mprocess_image\u001b[0;34m(image, objects, groundtruth, mode)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mimage_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_inference_for_single_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetection_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mfilter_detections\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-453-4110867dcb70>\u001b[0m in \u001b[0;36mrun_inference_for_single_image\u001b[0;34m(model, image)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# Run inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# All outputs are batches tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1653\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mdo\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m     \"\"\"\n\u001b[0;32m-> 1655\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m       return super(WrappedFunction, self)._call_impl(\n\u001b[0;32m--> 247\u001b[0;31m           args, kwargs, cancellation_manager)\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_signature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1671\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mstructured_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1673\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_with_flat_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_with_flat_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_with_flat_signature\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1720\u001b[0m                         \"got {} ({})\".format(self._flat_signature_summary(), i,\n\u001b[1;32m   1721\u001b[0m                                              type(arg).__name__, str(arg)))\n\u001b[0;32m-> 1722\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_with_structured_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "import proglog\n",
    "proglog.notebook()\n",
    "\n",
    "mot_tracker = Sort( max_age=8, iou_threshold=0.5)\n",
    "\n",
    "tracked_ids=None\n",
    "warning_ids = []\n",
    "frame = 0\n",
    "write_output = 'output_' + model_name + str(time.time())+ '.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first n seconds\n",
    "# clip1 = VideoFileClip(\"test.mp4\").subclip(0,1)\n",
    "\n",
    "clip1 = VideoFileClip(\"accident_compilation.mp4\")\n",
    "\n",
    "\n",
    "white_clip = clip1.fl_image(process_image) \n",
    "\n",
    "white_clip.write_videofile(write_output, audio=False, verbose=False)\n",
    "\n",
    "clip1.close()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "kernel4b7879e069 (3).ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
