{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hazard Detection for self-driving vehicles using a monocular camera\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Tensorflow Dataset\n",
    "\n",
    "* `tfds-nightly`: Released every day, contains the last versions of the datasets.\n",
    "\n",
    "\n",
    "Note: TFDS requires `tensorflow` (or `tensorflow-gpu`) to be already installed. TFDS support TF >=1.15.\n",
    "\n",
    "This notebook uses `tfds-nightly` and TF 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AfewpZcfzLgR"
   },
   "outputs": [],
   "source": [
    "!pip install -U tfds-nightly --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install prerequisites: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7S9DAboIJ3wU"
   },
   "outputs": [],
   "source": [
    "!pip install tf_slim pycocotools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOsfU9Xv3Mps"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fetches the tensorflow/models directory from github and compliles photobuts and the object_detection package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "461lqB2veb-1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "  while \"models\" in pathlib.Path.cwd().parts:\n",
    "    os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "  !git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd models/research/\n",
    "protoc object_detection/protos/*.proto --python_out=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd models/research\n",
    "pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the tensorflow object detection API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7LPTyzcngy4t"
   },
   "outputs": [],
   "source": [
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CHtRKeWLib4y"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VxFBTsrlg8DF"
   },
   "outputs": [],
   "source": [
    "!gcloud config set account application-default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm hosting a clone of the KITTI dataset in Google Cloud Storage. \n",
    "\n",
    "It can be used seamlessly without having to download the ~11.36GB dataset (recommended)\n",
    "\n",
    "If you would like to have a local copy of the dataset. Change the `data_dir` parameter to a local folder and chance the download argument to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pm0HljBV5PCg"
   },
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'kitti',\n",
    "    split=['train', 'test'],\n",
    "    with_info=True,\n",
    "    download=False,\n",
    "#     data_dir='./tensorflow_datasets'\n",
    "    data_dir=\"gs://kitti-dataset-1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model preparation\n",
    "\n",
    "By default we use an \"Faster R-CNN ResNet50 Low proposals\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z23bQEmKZY3D"
   },
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "  base_url = 'http://download.tensorflow.org/models/object_detection/'\n",
    "  model_file = model_name + '.tar.gz'\n",
    "  model_dir = tf.keras.utils.get_file(\n",
    "    fname=model_name, \n",
    "    origin=base_url + model_file,\n",
    "    untar=True)\n",
    "\n",
    "  model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
    "\n",
    "  model = tf.saved_model.load(str(model_dir))\n",
    "  model = model.signatures['serving_default']\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7porMrE1YnMq"
   },
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'faster_rcnn_resnet50_lowproposals_coco_2018_01_28'\n",
    "# model_name = 'faster_rcnn_resnet101_lowproposals_coco_2018_01_28'\n",
    "\n",
    "detection_model = load_model(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Label map\n",
    "\n",
    "Label maps map indices to category names, so that when our convolution network predicts 1, we know that this corresponds to car. Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u7AC_EM6dja5"
   },
   "outputs": [],
   "source": [
    "# List of the strings that is used to add correct label for each box.\n",
    "\n",
    "# PATH_TO_LABELS = 'models/research/object_detection/data/kitti_label_map.pbtxt'\n",
    "PATH_TO_LABELS = 'models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZvLqBFuq5h3"
   },
   "outputs": [],
   "source": [
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCEmzEc_rGdN"
   },
   "outputs": [],
   "source": [
    "\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GOyGpImZOe5-"
   },
   "outputs": [],
   "source": [
    "print(detection_model.inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hRQW2QSmd_vf"
   },
   "outputs": [],
   "source": [
    "detection_model.output_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FMI1eV4-eCY2"
   },
   "outputs": [],
   "source": [
    "detection_model.output_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output format\n",
    "\n",
    "\n",
    "- Output classes are always integers in the range 0, num_classes). Any mapping of these integers to semantic labels is to be handled outside of this class. We never explicitly emit a “background class” --- thus 0 is the first non-background class and any logic of predicting and removing implicit background classes must be handled internally by the implementation.\n",
    "\n",
    "\n",
    "- Detected boxes are to be interpreted as being in (y_min, x_min, y_max, x_max) format and normalized relative to the image window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper method that runs an inference for a single image for the selected model. Adds a wrapper function to call the model, and cleanup the outputs:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z56oS5LSg8D_"
   },
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image):\n",
    "  image = np.asarray(image)\n",
    "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "  input_tensor = tf.convert_to_tensor(image)\n",
    "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "  input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "  # Run inference\n",
    "  output_dict = model(input_tensor)\n",
    "\n",
    "  # All outputs are batches tensors.\n",
    "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "  # We're only interested in the first num_detections.\n",
    "  num_detections = int(output_dict.pop('num_detections'))\n",
    "  output_dict = {key:value[0, :num_detections].numpy() \n",
    "                 for key,value in output_dict.items()}\n",
    "  output_dict['num_detections'] = num_detections\n",
    "\n",
    "  # detection_classes should be ints.\n",
    "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "   \n",
    "  # Handle models with masks:\n",
    "  if 'detection_masks' in output_dict:\n",
    "    # Reframe the the bbox mask to the image size.\n",
    "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "               image.shape[0], image.shape[1])      \n",
    "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
    "                                       tf.uint8)\n",
    "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizes a dataset of bounding boxes according to the KITTI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(data):\n",
    "    height= 375\n",
    "    width = 1242\n",
    "    \n",
    "    data.loc[:,'xmin'] = data['xmin'] / width \n",
    "    data.loc[:,'xmax'] = data['xmax'] / width\n",
    "    data.loc[:,'ymax'],data.loc[:,'ymin']  = (height - data['ymin']) / height , (height - data['ymax']) / height\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maps COCO class IDs to KITTI class IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_kitti(classes):\n",
    "    \n",
    "    hash = { 3:1 , 8:3, 1:4, 7:7 }\n",
    "    return [ hash.get(classes[i], 8) for i in range(len(classes)) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes detections that are below a minimum threshold and also classifications that are outside the domain of the KITTI dataset since COCO has many classes we don't care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_detections( output_dict):\n",
    "    \n",
    "    output_dict['detection_classes'] = transform_to_kitti(output_dict['detection_classes'])\n",
    "    \n",
    "#     print(output_dict  )\n",
    "    misc_ids = (8,9)\n",
    "    \n",
    "    scores = output_dict['detection_scores']\n",
    "    classes = output_dict['detection_classes']\n",
    "    \n",
    "    size = len(classes)\n",
    "    min_threshhold = 0.5\n",
    "    output_dict['detection_boxes'] = np.array([ output_dict['detection_boxes'][i] for i in range(size) if scores[i] >= min_threshhold and classes[i] not in misc_ids  ])\n",
    "    output_dict['detection_classes'] = np.array([ output_dict['detection_classes'][i] for i in range(size) if scores[i] >= min_threshhold and classes[i] not in misc_ids  ])\n",
    "    output_dict['detection_scores'] = np.array([ output_dict['detection_scores'][i] for i in range(size) if scores[i] >= min_threshhold and classes[i] not in misc_ids  ])\n",
    "\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two helper methods are for formatting bounding boxes for the SORT algorithm which takes  a numpy array of detections in the format `[[x1,y1,x2,y2,score],[x1,y1,x2,y2,score],...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_for_sort(array):\n",
    "\n",
    "     return np.array( [array[1] ,array[0], array[3], array[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_detections_for_mot(outputdict):\n",
    "    \n",
    "    detections = [ np.append( reorder_for_sort(outputdict['detection_boxes'][i]) , outputdict['detection_scores'][i])  for i in range( len(outputdict['detection_classes'])) ] \n",
    "    \n",
    "    return np.asarray(detections)  if len(detections) else np.empty((0, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KITTI measures it's coordinates from the top-left as opposed to bottom-left hence we need to re-format this to the prevailing format in this application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_boxes( boxes):\n",
    "    \n",
    "    result = boxes.numpy()\n",
    "    for box in result:\n",
    "        box[0]=1-box[0]\n",
    "        box[2]=1-box[2]\n",
    "        box[0],box[2] = box[2], box[0]\n",
    "    return tf.convert_to_tensor(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tracked_color(label):\n",
    "    \n",
    "    return vis_util.STANDARD_COLORS[label %len(vis_util.STANDARD_COLORS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the length of a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length(x1,y1,x2,y2):\n",
    "    return sqrt( (x2-x1)**2 + (y2-y1)**2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Location estimator\n",
    "\n",
    "Training for this model is performed in LocNet notebook and saved to disk in generated_files folder. Here we load the model we've trained to be used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "MODEL = \"model@1597671595\"\n",
    "WEIGHTS = \"model@1597671595\"\n",
    "\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('generated_files/{}.json'.format(MODEL), 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json( loaded_model_json )\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"generated_files/{}.h5\".format(WEIGHTS))\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "loaded_model.compile(loss='mean_squared_error', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categories = [{\n",
    "        'id': 1,\n",
    "        'name': 'car'\n",
    "    }, {\n",
    "        'id': 2,\n",
    "        'name': 'van'\n",
    "    }, {\n",
    "        'id': 3,\n",
    "        'name': 'truck'\n",
    "    }, {\n",
    "        'id': 4,\n",
    "        'name': 'pedestrian'\n",
    "    }]\n",
    "\n",
    "cat_index  = {i+1: val for i,val in enumerate(categories) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_locnet( bboxes):\n",
    "  \n",
    "    if len(bboxes)==0 or len(bboxes)==1 and not any(bboxes[0]):\n",
    "        return []\n",
    "    y_pred = loaded_model.predict(bboxes)\n",
    "  \n",
    "    return np.hstack((bboxes,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install filterpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializae SORT Object\n",
    "\n",
    "Refer to [SORT documentation](https://github.com/abewley/sort) for internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sort import *\n",
    "mot_tracker = Sort( max_age=8, iou_threshold=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference and Visualizations\n",
    "\n",
    "This is the main block that performs operations for \n",
    "1. Object Detection\n",
    "2. Multiple Object Tracking and Trajectory prediction\n",
    "3. 3D location estimation\n",
    "\n",
    "The mode run is determined by the 'mode' argument to the function, defaults to object detection. Takes in an image as input, runs the underlying model and writes visualizations to the image and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import cv2\n",
    "import PIL.Image as Image\n",
    "import PIL.ImageColor as ImageColor\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "import PIL.ImageFont as ImageFont\n",
    "from math import sqrt\n",
    "\n",
    "def process_image(image, objects=None, groundtruth=None, mode=None):\n",
    "  \n",
    "\n",
    "    image_np = np.copy(image)\n",
    "    \n",
    "    output_dict = run_inference_for_single_image(detection_model, image_np)\n",
    "    \n",
    "    output_dict =filter_detections( output_dict)\n",
    "\n",
    "    max_age=25\n",
    "    global frame\n",
    "    global warning_ids\n",
    "    thickness=1\n",
    "    if mode =='tracking':\n",
    "        \n",
    "        detections = format_detections_for_mot(output_dict)\n",
    "\n",
    "        tracked_objects = mot_tracker.update(detections) #Run SORT algorithm for all detections made per frame\n",
    "        \n",
    "        #Accessing each KalmanTrackerBox internally to run the trajectory prediction steps outside of the Multiple Object Tracker. We filter to get only active trackers\n",
    "        live_trackers = (i for i in mot_tracker.trackers if i.id+1 in (int(j[4]) for j in tracked_objects) )\n",
    "        \n",
    "        for kt in live_trackers:\n",
    "            \n",
    "            k = copy.deepcopy(kt) \n",
    "            center = convert_bbox_to_z(k.get_state()[0]).T[0] #Gets the center coordinates of the tracker\n",
    "\n",
    "            for x in range(60): # Run prediction for 60 steps (equivalent to 60 frames in the future sequentially)\n",
    "                predicted = k.predict()\n",
    "\n",
    "            xmin,ymin,xmax,ymax = predicted[0] \n",
    "\n",
    "            predicted_center = convert_bbox_to_z(predicted[0]).T[0] #Center of the predicted point of the object\n",
    "            \n",
    "            if center[2] < 0.4: #Filters out detections that are too large, usally the dashboard of the car or window frames are caught incorrectly.\n",
    "\n",
    "                image = Image.fromarray(image_np)\n",
    "                draw = ImageDraw.Draw(image)\n",
    "                im_width, im_height = image.size\n",
    "\n",
    "\n",
    "                (x1,y1) = center[:2]\n",
    "                (x2,y2) = predicted_center[:2]\n",
    "\n",
    "                (x1_absolute, x2_absolute, y1_absolute, y2_absolute) = (x1 * im_width, x2 * im_width,\n",
    "                                      y1 * im_height, y2 * im_height)\n",
    "\n",
    "                #Draws the trajectory line from the prediction made with the Kalman Filter\n",
    "                draw.line([(x1_absolute, y1_absolute), (x2_absolute, y2_absolute)],\n",
    "                          width=3,\n",
    "                          fill='red')\n",
    "                \n",
    "                np.copyto(image_np, np.array(image))\n",
    "            \n",
    "                #Hazard detection step. Checks if object falls within set thresholds\n",
    "                if 0.2 < predicted_center[0] < 0.8 and 0.75 < predicted_center[1] and length(x1,y1,x2,y2) > sqrt(center[2])*0.6:\n",
    "                    warning_ids.append( (k.id+1,frame+max_age))\n",
    "            \n",
    "            \n",
    "        warning_ids = [i for i in warning_ids if i[1]>frame ] #Warnings should persit for max_age frames, filter expired warnings here\n",
    "        \n",
    "        for object in tracked_objects:\n",
    "            xmin,ymin,xmax,ymax = object[:4]\n",
    "            track_id = int(object[4])\n",
    "            n= track_id%40 +1\n",
    "#             n= track_id\n",
    "            label = f'object{n:03}'\n",
    "            \n",
    "            color = get_tracked_color(track_id)\n",
    "            \n",
    "            #Visualize Boxes and WARNING for an object that has been detected as a hazard\n",
    "            if convert_bbox_to_z(object).T[0][2] < 0.4:\n",
    "                if track_id in (i[0] for i in warning_ids ):\n",
    "                    color = 'red'\n",
    "                    label = 'WARNING!'\n",
    "                    thickness=4\n",
    "                vis_util.draw_bounding_box_on_image_array(image_np, ymin, xmin, ymax, xmax, thickness=thickness, display_str_list=[label],color=color)\n",
    "\n",
    "\n",
    "    elif mode=='locations':\n",
    "        #Runs the 3D location estimator and prints visualizations\n",
    "        locations = run_locnet( output_dict['detection_boxes'])\n",
    "\n",
    "        i=0\n",
    "        while i < len(objects['location']) and i < len(locations):\n",
    "            ymin,xmin,ymax,xmax = locations[i][:4]\n",
    "            coords = locations[i][4:7:2]\n",
    "            groundtruth_coords = objects['location'].numpy()[i][0:3:2]\n",
    "            label = \"predicted:(%.1f,%.1f), actual:(%.1f,%.1f)\" % tuple(np.append(coords  , groundtruth_coords).tolist())\n",
    "            vis_util.draw_bounding_box_on_image_array(image_np, ymin, xmin, ymax, xmax, thickness=1, display_str_list=[label],color='green')\n",
    "            i+=1\n",
    "\n",
    "            \n",
    "    else:\n",
    "        # Visualization of the results of object detection detection.\n",
    "        # If the grountruth flag is set visualize groundtruth boxes in addition\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np,\n",
    "          output_dict['detection_boxes'],\n",
    "          output_dict['detection_classes'],\n",
    "          output_dict['detection_scores'],\n",
    "          cat_index,\n",
    "          use_normalized_coordinates=True,\n",
    "          line_thickness=2)\n",
    "\n",
    "\n",
    "        if groundtruth:\n",
    "            groundtruth_boxes = format_boxes(objects['bbox']).numpy()\n",
    "            groundtruth_classes = objects['type'].numpy()+1\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np,\n",
    "          groundtruth_boxes,\n",
    "          groundtruth_classes,\n",
    "            None,\n",
    "          cat_index,\n",
    "          use_normalized_coordinates=True,\n",
    "          line_thickness=1,\n",
    "            groundtruth_box_visualization_color='blue')\n",
    "        \n",
    "    frame +=1\n",
    "    return image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model an image and show the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZ32ZyN7g8EA"
   },
   "outputs": [],
   "source": [
    "def show_inference(model, tensor, objects, groundtruth=None, mode=None):\n",
    "  \n",
    "    image_np = np.array(tensor)\n",
    "    image =process_image(image_np, objects, groundtruth, mode)\n",
    "    display(Image.fromarray(image))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Location prediction experiments\n",
    "\n",
    "_(GPU required)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracked_ids=None\n",
    "warning_ids = []\n",
    "frame = 0\n",
    "for example in ds_test.take(5):  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
    "    image = example[\"image\"]\n",
    "    objects = example[\"objects\"]\n",
    "  \n",
    "    print('bbox:' ,objects['bbox'])\n",
    "    print('location:', objects['location'])\n",
    "    print('type:', objects['type'])\n",
    "    show_inference(detection_model, image, objects, mode='locations')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Time experiments\n",
    "\n",
    "_Takes a while.. _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ffJh1zAkg8EC"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "total = 0\n",
    "for example in ds_test:  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
    "    image = example[\"image\"]\n",
    "    objects = example[\"objects\"]\n",
    "  \n",
    "    start = time.time()\n",
    "    image_np = np.array(image)\n",
    "    run_inference_for_single_image(detection_model, image_np)\n",
    "    end = time.time()\n",
    "    total += (end - start)\n",
    "    \n",
    "print( total/len(ds_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative Evaluation Steps\n",
    "\n",
    "Shows side-by-side comparisons for Object Detections vs groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from object_detection import eval_util\n",
    "from object_detection.core import standard_fields as fields\n",
    "from object_detection.metrics import coco_evaluation\n",
    "from object_detection.protos import eval_pb2\n",
    "from object_detection.utils import test_case\n",
    "from object_detection.utils import tf_version\n",
    "\n",
    "input_data_fields = fields.InputDataFields\n",
    "detection_fields = fields.DetectionResultFields\n",
    "\n",
    "\n",
    "for example in ds_test.take(5):  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
    "    image = example[\"image\"]\n",
    "    objects = example[\"objects\"]\n",
    "#     show_inference(detection_model, image)\n",
    "    image_np = np.array(image)\n",
    "    # Actual detection.\n",
    "    output_dict = run_inference_for_single_image(detection_model, image_np)\n",
    "\n",
    "\n",
    "    output_dict = filter_detections( output_dict)\n",
    "\n",
    "    batch_size = 1\n",
    "    key=tf.constant('image1')\n",
    "    \n",
    "    groundtruth_boxes = format_boxes(objects['bbox'])\n",
    "    groundtruth_classes = objects['type']+1\n",
    "    groundtruth = {\n",
    "        input_data_fields.groundtruth_boxes: groundtruth_boxes,\n",
    "        input_data_fields.groundtruth_classes: groundtruth_classes,      \n",
    "    }\n",
    "    \n",
    "    num_detections = tf.constant([len(output_dict['detection_classes'])])\n",
    "    \n",
    "    detections = {\n",
    "        detection_fields.detection_boxes: tf.constant([output_dict['detection_boxes']]) ,\n",
    "        detection_fields.detection_scores: tf.constant([output_dict['detection_scores']]),\n",
    "        detection_fields.detection_classes: tf.constant([output_dict['detection_classes']-1]),\n",
    "        detection_fields.num_detections: num_detections\n",
    "       \n",
    "    }\n",
    "    \n",
    "\n",
    "    image = tf.constant([image_np])\n",
    "    \n",
    "    \n",
    "    result_dict = eval_util.result_dict_for_single_example(image, key,detections, groundtruth)\n",
    "    \n",
    "    side_by_side_img =  vis_util.draw_side_by_side_evaluation_image(result_dict,cat_index)[0][0].numpy()\n",
    "    display(Image.fromarray(side_by_side_img))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative Evaluation Steps (PASCAL VOC metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.core import standard_fields\n",
    "from object_detection.utils import object_detection_evaluation\n",
    "\n",
    "\n",
    "categories = [{\n",
    "        'id': 1,\n",
    "        'name': 'car'\n",
    "    }, {\n",
    "        'id': 4,\n",
    "        'name': 'pedestrian'\n",
    "    }]\n",
    "\n",
    "pascal_evaluator = object_detection_evaluation.PascalDetectionEvaluator(\n",
    "        categories,matching_iou_threshold=0.5)\n",
    "\n",
    "for index, example in enumerate(ds_test):  \n",
    "    image = example[\"image\"]\n",
    "    objects = example[\"objects\"]\n",
    "  \n",
    "    image_np = np.array(image)\n",
    "    # Actual detection.\n",
    "    output_dict = run_inference_for_single_image(detection_model, image_np)\n",
    "    output_dict = filter_detections( output_dict)\n",
    "    \n",
    "    image_key = 'img'+str(index)\n",
    "    \n",
    "    groundtruth_boxes = format_boxes(objects['bbox']).numpy()\n",
    "    groundtruth_classes = objects['type'].numpy()+1\n",
    "\n",
    "\n",
    "    pascal_evaluator.add_single_ground_truth_image_info(\n",
    "        image_key,\n",
    "        {standard_fields.InputDataFields.groundtruth_boxes: groundtruth_boxes,\n",
    "         standard_fields.InputDataFields.groundtruth_classes:\n",
    "         groundtruth_classes,\n",
    "        })\n",
    "\n",
    "    if len(output_dict['detection_classes']):\n",
    "        pascal_evaluator.add_single_detected_image_info(\n",
    "                image_key,\n",
    "                {standard_fields.DetectionResultFields.detection_boxes: output_dict['detection_boxes'],\n",
    "                 standard_fields.DetectionResultFields.detection_scores:\n",
    "                 output_dict['detection_scores'],\n",
    "                 standard_fields.DetectionResultFields.detection_classes:\n",
    "                 output_dict['detection_classes']\n",
    "                })\n",
    "    \n",
    "\n",
    "    metrics = pascal_evaluator.evaluate()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install moviepy proglog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the line below of the accident_compilation file doesn't exist in the project folder (Esp if you downloaded this from my github)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil cp gs://hazard-detection-test-videos/accident_compilation.mp4 accident_compilation.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runs model on actual video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "import proglog\n",
    "proglog.notebook()\n",
    "\n",
    "mot_tracker = Sort( max_age=8, iou_threshold=0.5)\n",
    "\n",
    "tracked_ids=None\n",
    "warning_ids = []\n",
    "frame = 0\n",
    "write_output = 'output_' + model_name + str(time.time())+ '.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first n seconds\n",
    "# clip1 = VideoFileClip(\"test.mp4\").subclip(0,1)\n",
    "\n",
    "clip1 = VideoFileClip(\"accident_compilation.mp4\")\n",
    "\n",
    "\n",
    "white_clip = clip1.fl_image(process_image) \n",
    "\n",
    "white_clip.write_videofile(write_output, audio=False, verbose=False)\n",
    "\n",
    "clip1.close()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "kernel4b7879e069 (3).ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
